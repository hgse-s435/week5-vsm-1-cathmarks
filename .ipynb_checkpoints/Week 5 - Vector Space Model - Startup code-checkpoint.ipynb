{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Vector Space Model (VSM) and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next weeks, we are going to re-implement Sherin's algorithm and apply it to the text data we've been working on last week! Here's our roadmap:\n",
    "\n",
    "**Week 5 - data cleaning**\n",
    "1. import the data\n",
    "2. clean the data (e.g., remopve stop words, punctuation, etc.)\n",
    "3. build a vocabulary for the dataset\n",
    "4. create chunks of 100 words, with a 25-words overlap\n",
    "5. create a word count matrix, where each chunk of a row and each column represents a word\n",
    "\n",
    "**Week 6 - vectorization and linear algebra**\n",
    "6. Dampen: weight the frequency of words (1 + log[count])\n",
    "7. Scale: Normalize weighted frequency of words\n",
    "8. Direction: compute deviation vectors\n",
    "\n",
    "**Week 7 - Clustering**\n",
    "9. apply different unsupervised machine learning algorithms\n",
    "    * figure out how many clusters we want to keep\n",
    "    * inspect the results of the clustering algorithm\n",
    "\n",
    "**Week 8 - Visualizing the results**\n",
    "10. create visualizations to compare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) using glob, find all the text files in the \"Papers\" folder\n",
    "# Hint: refer to last week's notebook\n",
    "import glob\n",
    "text_files = glob.glob('Papers/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) get all the data from the text files into the \"documents\" list\n",
    "# P.S. make sure you use the 'utf-8' encoding\n",
    "documents = []\n",
    "for file in text_files:\n",
    "    f= open(file,\"r\",encoding='utf-8')\n",
    "    f = f.read()\n",
    "    documents.append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "\n",
      "\f",
      "epistemic network analysis and topic modeling for chat\n",
      "data from collaborative learning environment\n",
      "zhiqiang cai\n",
      "\n",
      "brendan eagan\n",
      "\n",
      "nia m. dowell\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "zcai@memphis.edu\n",
      "\n",
      "eaganb@gmail.com\n",
      "\n",
      "niadowell@gmail.com\n",
      "\n",
      "james w. pennebaker\n",
      "\n",
      "david w. shaffer\n",
      "\n",
      "arthur c. graesser\n",
      "\n",
      "university of texas-austin\n",
      "116 inner campus dr stop g6000\n",
      "austin, tx, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 403\n",
      "memphis, tn, usa\n",
      "\n",
      "pennebaker@utexas.edu\n",
      "\n",
      "dws@education.wisc.edu\n",
      "\n",
      "art.graesser@gmail.com\n",
      "\n",
      "abstract\n",
      "this study investigates a possible way to analyze chat data from\n",
      "collaborative learning environments using epistemic network\n",
      "analysis and topic modeling. a 300-topic general topic model\n",
      "built from tas\n"
     ]
    }
   ],
   "source": [
    "# 3) print the first 1000 characters of the first document to see what it \n",
    "# looks like (we'll use this as a sanity check below)\n",
    "print(documents[0][0:999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) only select the text that's between the first occurence of the \n",
    "# the word \"abstract\" and the last occurence of the word \"reference\"\n",
    "# Optional: print the length of the string before and after, as a \n",
    "# sanity check\n",
    "# HINT: https://stackoverflow.com/questions/14496006/finding-last-occurrence-of-substring-in-string-replacing-that\n",
    "# read more about rfind: https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "\n",
    "\n",
    "start = documents[0].find('abstract')\n",
    "end = documents[0].rfind('reference')\n",
    "documents[0] = documents[0][start:end]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling. a 300-topic general topic model built from tasa (touchstone applied science associates) corpus was used in this study. 300 topic scores for each of the 15,670 utterances in our chat data were computed. seven relevant topics were selected based on the total document scores. while the aggregated topic scores had some power in predicting students‚Äô learning, using epistemic network analysis enables assessing the data from a different angle. the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different (ùë° = 2.00). overall, the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions.  keywords chat; collaborative learning; topic modeli'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) replace carriage returns (i.e., \"\\n\") with a white space\n",
    "# check that the result looks okay by printing the \n",
    "# first 1000 characters of the 1st doc:\n",
    "documents[0] = documents[0].replace('\\n', ' ')\n",
    "documents[0][0:999]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a 300 topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study  300 topic scores for each of the 15 670 utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  ùë°   2 00   overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modeli'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) replace the punctation below by a white space\n",
    "# check that the result looks okay \n",
    "# (e.g., by print the first 1000 characters of the 1st doc)\n",
    "\n",
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '‚àí', '‚Äù', '‚Äú', '‚Äô']\n",
    "\n",
    "for item in punctuation:\n",
    "    documents[0] = documents[0].replace(item, ' ')\n",
    "documents[0][0:999]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a  topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study   topic scores for each of the   utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  ùë°       overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) remove numbers by either a white space or the word \"number\"\n",
    "# again, print the first 1000 characters of the first document\n",
    "# to check that you're doing the right thing\n",
    "\n",
    "table = str.maketrans(\"\", \"\", \"0123456789\")\n",
    "documents[0] = documents[0].translate(table)\n",
    "documents[0][0:999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling   topic general topic model built tasa  touchstone applied science associates  corpus used study   topic scores   utterances chat data computed  seven relevant topics selected based total document scores  aggregated topic scores power predicting students  learning  using epistemic network analysis enables assessing data different angle  results showed topic score based epistemic networks low gain students high gain students significantly different  ùë°       overall  results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic network analysis    introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously participate tasks accomplish shared learning goals       collaborative learning groups range pair learners  called dyad   small groups    learners   classroom learning    learners   recently large scale online learning environments hundreds even thousands students       collaborative process provides learners efficient learning experience improves learners  collaborative learning skills  critical competencies students     members team different many ways  experience  knowledge  skills  approaches learning  student col   laborative learning environment take students  views ideas information provided learning material  ideas coming team integrated deeper understanding material  better solution problem  traditional collaborative learning occurred form face face group discussion problem solving  internet learning technologies develop  online collaborative learning environments come playing more important roles  example  moocs  massive open online courses  drawn massive number learners  learners moocs connected internet easily interact using various types tools  forums  blogs social networks     digitized environments make possible track learning processes collaborative learning environments greater detail  communication one main factors differentiates collaborative learning individual learning          chats collaborative learning environments provide rich data contains information dynamics learning process  understanding massive chat data collaborative learning environments interesting challenging  many tools invented used chat data analysis  liwc  linguistic inquiry word count      coh metrix     topic modeling  name  epistemic network analysis  ena  playing unique role analyzing chat data epistemic games     ena rooted specific theory learning  epistemic frame theory  collection skill  knowledge  identity  value epistemology  skive  forms epistemic frame  critical theoretical assumption ena connections elements epistemic frames critical learning  presence isolation  online ena toolkit allows users analyze chat data comparing connections within epistemic networks derived chats  ena visualization displays clustering learners groups network connections individual learners groups  ena requires coded data traditionally relied hand coded data sets classifiers rely regular expression mapping  combining topic modeling ena provide new mode preparing data sets analysis using ena  study  used combination topic modeling ena analyze chat data see could detect differences connections made students high learning gains versus students low learning gains  incorporating topic modeling  \\x0cwith ena make analytic tool fully automated greater use research community     related work chats two obvious features  first  appear form text  therefore  text analysis tool may role chat analysis  second  chats come individuals  interaction  reflects social dynamics participants  therefore  combination text analysis social network analysis helpful understanding underlying chat dynamics  instance  tuulos et al     combined topic modeling social network analysis chat data analysis  found topic modeling help identify receiver chats  person chat given   similar effort  scholand et al     combined liwc social network analysis form method called  social language network analysis   slna   social networks formed counting number times chat occurred two participants  based counts  participants clustered tree structure  representing level subgroups participants belong  liwc used get text features chats  found  liwc features significantly different group conversations group conversations  researchers also recently explored advantages combining sna  social network analysis  deeper level computational linguistic tools  like coh metrix  coh metrix computes  text features  five important coh metrix features  narrativity  syntax simplicity  word concreteness  referential cohesion deep cohesion  dowell colleagues    explored extent characteristics discourse diagnostically reveals learners  performance social position moocs  found learners performed significantly better engaged expository style discourse  surface deep level cohesive integration  abstract language  simple syntactic structures  however  linguistic profiles centrally positioned learners differed high performers  learners significant central position social network engaged using narrative style discourse less overlap words ideas  simpler syntactic structures abstract words  increasing methodological contribution work highlights automated linguistic analysis student interactions complement social network analysis  sna  techniques adding rich contextual information structural patterns learner interactions   final sample  within population     sample identified caucasian     hispanic latino     asian american     african american  less   identified either native american pacific islander  course details procedure  students told would participating assignment involved collaborative discussion personality disorders taking quizzes  students told assignment log online educational platform specific university specified time  would take quizzes interact via web chat one four random group members  students also instructed  prior logging onto educational platform  would read material personality disorders  logging system  students took  item  multiple choice pretest quiz  quiz asked students apply knowledge personality disorders various scenarios draw conclusions based nature disorders  following example types quiz questions students exposed  \\uf0b7 \\uf0b7  \\uf0b7  jacob diagnosed narcissistic personality disorder  might dr  simon think wrong diagnosis  dr  level measured described  mice varying ages terms length  cm  weight  g   might describe characteristics using dimensional approach  danielle checks facebook page every hour  danielle narcissistic personality disorder   completing quiz  randomly assigned students waiting engage chatroom portion task  least  students  students  m       individuals directed instant messaging platform built educational platform  group chat began soon someone typed first message lasted  minutes  chat window closed automatically  minutes  time students took second  multiplechoice question quiz  student contributed   words average  sd        sentences  sd       group  discussions   words long  sd        sentences long  sd       excerpt collaborative interaction chat chat room shown table    student names changed   table   excerpt collaborative interaction chat student  chat text  another study  dowell et al     showed students  linguistic characteristics  namely higher degrees narrativity deep cohesion  predictive learning   students engaged deep cohesive interactions performed better   art  ok cool  everyone  sooo first question  art  ok certain characteristics considered personality disorder   present research  explore collaborative interaction chat data using combination topic modeling epistemic network analysis  previous studies focused relationship language features social network connections  study focuses prediction learning performance semantic network connections students make chats   shaffer  alright sooo first question  based criteria describe several reasons psychologist might label someone grandiose thoughts narcissistic personality disorder   shaffer  hahaha never mind  shaffer  second question     methods  art  lol good  shaffer  okay certain characteristics  doesn like stable thing   carl  think main thing disorder disruptive socially makes person danger others  participants  participants enrolled introductory level psychology course taught fall semester  large university usa   students participated course  minor data loss occurred removing outliers failed complete outcome measures  final sample consisted  students  females made     \\x0cvasile  yes  stable time  shaffer  yeah  also mentioned drugs  art  also like unrealistic fantasies  nia  yeah normal culture  carl  drugs physical injury  vasile  begins early adulthood adolescence  shaffer  think covers  haha  art  ok  arrogance doesn define  characteristics  art  yeah think got  shaffer  like    excerpt  see several obvious things  first  lengths utterances varied one single word multiple sentences  needs considered text analysis methods work longer texts  example  coh metrix usually works well texts  words  topic modeling also needs enough length reliably infer topic scores  second  number utterances participant gave different  much member said  see member played different role chat  third  ordered sequence utterances forms time series  understanding visualizing underlying discourse dynamics important meaning making type data  data set contained   utterances  pretest scores  first quiz  post test scores  second quiz   students  grouped  chat rooms  chat room   students    average  average speech turns student gave   average speech turns room    average pretest score    correct average post test scores    correct  paired sample test shows post test significantly higher  ùë°      ùëÅ      computed learning gain student  using formula ùëîùëéùëñùëõ    ùëùùëúùë†ùë°ùë°ùëíùë†ùë° ùë†ùëêùëúùëüùëí   ùëùùëüùëíùë°ùëíùë†ùë° ùë†ùëêùëúùëüùëí  ùëùùëüùëíùë°ùëíùë†ùë° ùë†ùëêùëúùëüùëí     students  ùëÅ      average learning gain       positive learning gains       scores   negative learning gains  surprisingly  students lower pretest scores higher learning gains greater potential learn  figure  shows average learning gain function pretest score        data set analyzed multiple studies  cade et al     analyzed cohesion chats found deep cohesion chats predicts students feeling power connectedness group  dowell et al     found coh metrix measures predicts learning  coh metrix measures describe common textual features content specific  example  cohesion text segments semantically linked  nothing text content  study  use topic modeling provide content dependent features use epistemic network analysis explore topics associated chats     topic modeling topic modeling widely used text analysis find topics text proportion amount topic contained  latent dirichlet allocation  lda       one popular methods topic modeling  lda uses generative process find topic representations  lda starts large document set ùê∑    ùëë   ùëë   ‚ãØ   ùëëùëö    word list ùëä    ùë§   ùë§   ‚ãØ   ùë§ùëõ   extracted document set  lda assumes document set contains certain number topics  say  k topics  document probability distribution k topics topic probability distribution given list words  document composed  word occurred document assumed drawn based document topic probability topic word probability  given corpus  document set  given number topics k  lda compute topic assignment word document  given topic  word probability distribution easily computed number times word assigned given topic  beauty topic modeling  top words   words highest probabilities topic  usually give meaningful interpretation topic  distributions underlying representation topics  top words usually used show topics contained corpus  counting number words assigned topic  topic proportion score computed document topic  topic proportion scores become document feature used analysis  however  proportion scores based statistical topic assignment words  documents short  utterances chat data  topic proportion scores won reliable  cai et al     argued alternative ways compute document topic scores possible     tasa topic model          students pretest scores less   correct  n    average learning gain       positive learning gains     scores    negative learning gains                            figure   average learning gain function pretest score   although chat data set contained   utterances  utterances short corpus large enough build reliable topic model  get reliable model  used well known corpus provided tasa  touchstone applied science associates   corpus contained documents seven known categories  including business  health  home economics  industrial arts  language arts  science social studies  content topic  personality disorders  obviously health category  course  topics tasa relevant study  therefore  building model  need select relevant topics  cover next sub section   \\x0cthere total   documents tasa corpus   words long  ran lda  filtered high frequency words low frequency words  high frequency words           etc   won contain much topic information  rare words won contribute meaningful statistics    words  might better say  terms   left filtering  model  topics constructed lda     topic score computation topic selection tasa topic model  computed word topic probabilities based number times word assigned  topics  thus  word represented  dimensional probability distribution vector  chat chat corpus  simply summed word probability vectors words appeared chat  gave us  topic scores chat  recall  chats associated reading material two quizzes  students free talk anything  content reading material quizzes set main chat topics   personality disorders   topic score                 figure   sorted topic scores topic selection  first thing needed investigate whether  hot  topics computation made sense  find  computed sum topic scores chats  topics sorted according total topic score  hottest topic total score higher   much higher second highest  less    examining top words  topic  illness   highly relevant personality disorders  six hot topics scored range     outdoors    biology    people social    education   healthcare   top words listed   \\uf0b7 \\uf0b7 \\uf0b7  \\uf0b7  \\uf0b7   illness    biology    psychology   healthcare  topics learning materials involved   education  topic education environment chat happened   outdoor   people social  task topics  get idea whether topic scores related learning gain  aggregated scores person computed correlation total topic score learning gain topic  interested looking students larger potential learn  removed data pretest score greater equal    leaving  students   results  table   showed topics significantly correlated learning gain  doesn seem great  seems suggest  whatever topic student talked  student talked  larger gain student obtained  real reason aggregation  topic scores summed  therefore  topic scores influenced chat length  correlation table  basically showed chat length effect  table   correlation total topic scores learning gain  n   pretest       \\uf0b7  \\uf0b7  person  animal  mental  response  positive  stress  personality  subject  reaction people social  joe  pete  mr  charlie  dad  frank  billy  tony  jerry   ll  mom   d  going   re  got  boys  looked  asked  paper  go education  students  teacher  teachers  child  children  student  school  education  schools  learning  parents  tests  test  program  teaching  behavior  skills  reading  team  information healthcare  patient  doctor  health  hospital  medical  dr  patients  nurse  disease  doctors  team  care  office  nursing  drugs  medicine  services  dental  diseases  help  illness  health  disease  patient  body  diseases  medical  stress  mental  physical  heart  doctor  problems  cause  person  patients  exercise  illness  problem  nurse  healthy outdoors  dog  energy  plants  earth  car  light  food  heat  words  animals  music  rock  language  children  air  uncle  city  sun  women  plant biology  cells  cell  genes  chromosomes  traits  color  organisms  sex  egg  species  gene  body  male  female  parents  nucleus  eggs  sperm  organism  sexual psychology  behavior  learning  theory  environment  feelings  sexual  physical  social  sex  human  research   topic  post test  pretest  gain  illness                 outdoors                 biology                 psychology                people social               education                 healthcare                remove chat length effect  simplest way divide scores number words  terms  chat  however  study  consistent subsequent analysis  normalized topic scores topic proportion scores dividing topic score utterance sum seven topic scores utterance  results  table   showed topic  people social  significant negative correlation learning gain  others significant direction would expect   illness    biology    psychology   healthcare  positively correlated gain scores   outdoors   people social  topics negatively correlated gains scores  observed almost correlation  education  topic  seems indicate aggregated topic scores limited power predicting learning  therefore  used ena examine connections association topics students discourse  \\x0cdevelop predictive model learning gains based use topics  table   correlation normalized topic proportion scores learning gain  n   pretest    topic  post test  pretest  gain  illness            outdoors              biology            psychology           people social                 education            healthcare              epistemic network analysis ena measures connections elements data represents dynamic network models  ena creates network models metric space enables comparison networks terms   difference graph highlights weighted connections one network differ another   b  statistics summarize weighted structure network connections  enabling comparisons many networks  ena originally developed model cognitive networks involved complex thinking  cognitive networks represent associations knowledge  skills  habits mind individual learners groups learners  study  used ena construct network models  individual student  constructed ena network using selected seven topic scores utterance student contributed group     process process creating ena models described detail elsewhere  e g          briefly describe ena models created based topic modeling  defined network nodes seven topics identified topic model  defined connections nodes  edges  strength co occurrence topics within moving stanza window  msw  size      model connections topics used products topic scores summed across chats msw   topic  topic scores summed across  chats msw  ena computed product summed topic loadings pair topics measure strength co occurrence  example  sum topics scores across five chats    illness      psychology      healthcare   scores would result three co occurrences   illness psychology    illnesshealthcare    psychology healthcare   scores          respectively  next ena created adjacency matrices student quantified co occurrences topics within students  discourse context chat group  subsequently  adjacency matrices treated vectors high dimensional space  dimension corresponds co occurrence pair topics  vectors normalized unit vectors  notice normalization removed effect chat length embedded topic scores  singular value decomposition  svd  performed dimensional reduction  ena projected vector student low dimensional space maximizes variance explained data  finally  nodes  networks  case correspond seven selected topics generated tasa corpus  placed low dimensional space  topic nodes placed using optimization algorithm overall distances centroids  centers mass networks  corresponding projected student locations minimized  critical feature ena node placements fixed   nodes network place units analysis  fixing location nodes allows meaningful comparisons networks terms connection patterns allow us interpret metric space  result  ena produced two coordinated representations     location student projected metric space  units analysis included model located     weighted network graphs student  explained student positioned space  ena also allows us compare mean network graphs mean position ena space different groups students  study  considered students high potential learn  e    students pretest score        correct   among students  compared networks low learning gain students  gain     ùëÅ   networks high learning gain students  gain    ùëÅ    compared groups using difference network graph  formed subtracting edge weights mean discourse network low gain group students mean discourse network high gain group  difference network graph shows us topic connections stronger group  addition  conducted test test difference group means     results figure  shows mean discourse networks students low gain scores  left  red   students high gain scores  right  blue   difference network graph  center  shows discourse patterns group differs  students low gains stronger connections  people social  topic topics except  illness   importantly  connection strongest low gain students compared high gain students  people social   outdoors   students high gain scores made stronger connections topics  illness    psychology    healthcare    biology    education   table   comparison centroids low gain high gain students  ùíë   ùüé  ùüéùüíùüï  ùíï   ùüê  ùüéùüé n  mean  sd  high gain          low gain           figure  shows centroids  centers mass  individual students  discourse networks means low gain score students red high gain score students blue  differences two groups significant x dimensions  see table    means differences saw figure  described statistically significant  words  high learning gain students  discourse towards right side ena space low learning gain students  discourse towards left side  indicates discourse students high learning gains made connections task topics   illness    psychology    healthcare    biology    education    discourse  \\x0clow gain students made connections task topics   people social   outdoors       discussion ena makes possible visualize chat dynamics help researchers gain deeper understanding going collaborative learning environment  differences topics students connect discourse predict learning outcomes  previous use ena relied human coded data use regular expressions classify data  utilizing topic modeling lead fully automated ena  making accessible wider group researchers allows ena used larger data sets  fact epistemic network predicts learning validates application ena  example  turn turn chat dynamics plotted trajectories  d space   topics placed  investigating trajectory patterns relationship learning socio affective components interesting future research directions  used general topic model study  many studies literature used lda topic modeling relatively small corpora  causes two problems    lda topic models built upon small corpora reliable  lda requires large number documents relatively large size document  inadequate corpus result misleading results    using topic model common would result arbitrary interpretation  example  representation  illness  different corpus could different  therefore  hard compare claims made  illness  across different studies  using reliable  common topic models set common language different studies   figure   mean discourse networks students low gain scores  left  red   students high gain scores  right  blue   difference network graph  center   chat utterances short  statistical inference algorithm contains high degree randomness short documents  extreme example  utterance single word  would result inferred topic proportion scores    one topic    others  problem     assigned topic certain degree uncertainty   topic    assigned could topic  aggregated analysis may sensitive uncertainty  detailed utterance utterance analysis would suffer  method computing topic scores based topic probability distribution word  treat topic distribution word vector  computing topic score  simple sum word vectors gives scores topics  pointed  summation algorithm length effect  therefore  topic scores used  removing length effects normalization necessary  article  use weighted sum suggested cai et al      comparing effect different weighting beyond scope paper  figure   discourse network centroids low gain score students red  high gain score students blue  topic scores documents usually inferred topic models  longer documents  topic scores used many applications  e g   text clustering      inferred topic proportion scores won useful analyzing chats need treat utterance unit analysis  useful  general topic model used  selecting topics relevant specific analysis becomes important  approach look total scores utterances find  hot  topics sorting total topic scores  study  quickly decreasing curve helped us select topics  believe would case studies using model containing far topics topics contained target data   \\x0calthough study started topic modeling capture   chats  association networks constructed epistemic network analysis actually turned      topics chats associated  conceptually similar cohesion features dowell    cade    used  topic modeling emphasizes content words  topic model built  stop words usually removed  interesting question  opposite  keep stop words remove content words  pennebaker  e g       laid foundational work direction  liwc tool pennebaker colleagues created provides hundred text measures counting non content words  liwc measures could provide different features epistemic network analysis reveal different aspects chat dynamics     acknowledgments research supported national science foundation  drk    drk     institute education sciences  rc   army research lab  winf     office naval research  n  c  n  c    opinions  findings  conclusions recommendations expressed material authors necessarily reflect views nsf  ies  dod  tutoring research group  trg  interdisciplinary research team comprised researchers psychology  computer science  departments university memphis  visit http   www autotutor org      '"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8) Remove the stop words below from our documents\n",
    "# print the first 1000 characters of the first document\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "\n",
    "for word in stop_words:\n",
    "    word = ' '+word+' '\n",
    "    documents[0] = documents[0].replace(word, ' ')\n",
    "\n",
    "# documents[0]=documents[0].split()\n",
    "# for item in stop_words:\n",
    "#     for word in documents[0]:\n",
    "#         if item==word:\n",
    "#             documents[0].remove(item)\n",
    "# documents[0]= str(documents[0])\n",
    "\n",
    "   \n",
    "# #     documents[0]=documents[0].replace(word, '')\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract study investigates possible analyze chat data collaborative learning environments using epistemic network analysis topic modeling   topic general topic model built tasa  touchstone applied science associates  corpus used study   topic scores   utterances chat data computed  seven relevant topics selected based total document scores  aggregated topic scores power predicting students  learning  using epistemic network analysis enables assessing data different angle  results showed topic score based epistemic networks gain students high gain students significantly different       overall  results suggest analytical approaches provide complementary information afford insights processes related successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic network analysis    introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously p'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9) remove words with one and two characters (e.g., 'd', 'er', etc.)\n",
    "# print the first 1000 characters of the first document\n",
    "import re\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "\n",
    "documents[0]=shortword.sub('', documents[0])\n",
    "documents[0][:999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) package all of your work above into a function that cleans a given document\n",
    "\n",
    "def clean_list_of_documents(documents):\n",
    "    \n",
    "    cleaned_docs = []\n",
    "    \n",
    "    for document in documents:\n",
    "        start = document.find('abstract')\n",
    "        end = document.rfind('reference')\n",
    "        document = document[start:end]\n",
    "        document = document.replace('\\n', ' ')\n",
    "        for item in punctuation:\n",
    "            document = document.replace(item, ' ')\n",
    "        table = str.maketrans(\"\", \"\", \"0123456789\")\n",
    "        document = document.translate(table)\n",
    "        for word in stop_words:\n",
    "            word = ' '+word+' '\n",
    "            document = document.replace(word, ' ')\n",
    "        document=shortword.sub('', document)\n",
    "        cleaned_docs.append(document)\n",
    "        \n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract study investigates possible analyze chat data collaborative learning environments using epistemic network analysis topic modeling   topic general topic model built tasa  touchstone applied science associates  corpus used study   topic scores   utterances chat data computed  seven relevant topics selected based total document scores  aggregated topic scores power predicting students  learning  using epistemic network analysis enables assessing data different angle  results showed topic score based epistemic networks gain students high gain students significantly different       overall  results suggest analytical approaches provide complementary information afford insights processes related successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic network analysis    introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously p'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11a) reimport your raw data using the code in 2)\n",
    "documents = []\n",
    "for file in text_files:\n",
    "    f= open(file,\"r\",encoding='utf-8')\n",
    "    f = f.read()\n",
    "    documents.append(f)\n",
    "        \n",
    "# 11b) clean your files using the function above\n",
    "cleaned_docs = clean_list_of_documents(documents)\n",
    "\n",
    "# 11c) print the first 1000 characters of the first document\n",
    "cleaned_docs[0][:999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Build your list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words (i.e., the vocabulary) is going to become the columns of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Describe why we need to figure out the vocabulary used in our corpus (refer back to Sherin's paper, and explain in your own words): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5402"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13) create a function that takes in a list of documents\n",
    "# and returns a set of unique words. Make sure that you\n",
    "# sort the list alphabetically before returning it. \n",
    "\n",
    "def get_vocabulary(documents):\n",
    "    voc = []\n",
    "    for document in documents:\n",
    "        for word in document.split():\n",
    "            if word not in voc:\n",
    "                voc.append(word)\n",
    "    voc.sort()\n",
    "    return voc\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "len(get_vocabulary(cleaned_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) what was the size of Sherin's vocabulary? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5402 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - transform your documents into 100-words chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) create a function that takes in a list of documents\n",
    "# and returns a list of 100-words chunk \n",
    "# (with a 25 words overlap between them)\n",
    "# Optional: add two arguments, one for the number of words\n",
    "# in each chunk, and one for the overlap size\n",
    "# Advice: combining all the documents into one giant string\n",
    "# and splitting it into separate words will make your life easier!\n",
    "\n",
    "def hundred_words(documents):\n",
    "    documents = ' '.join(documents)\n",
    "    documents=documents.split()\n",
    "    n=100\n",
    "    hundred_word_list = []\n",
    "    while n<len(documents):\n",
    "        hundred_word_list.append(documents[n-100:n])\n",
    "        n+=25 \n",
    "    return hundred_word_list\n",
    "hundred_words = hundred_words(cleaned_docs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) create a for loop to double check that each chunk has \n",
    "# a length of 100\n",
    "# Optional: use assert to do this check\n",
    "for chunk in hundred_words:\n",
    "    if len(chunk)!=100:\n",
    "        print('not 100!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract',\n",
       " 'study',\n",
       " 'investigates',\n",
       " 'possible',\n",
       " 'analyze',\n",
       " 'chat',\n",
       " 'data',\n",
       " 'collaborative',\n",
       " 'learning',\n",
       " 'environments',\n",
       " 'using',\n",
       " 'epistemic',\n",
       " 'network',\n",
       " 'analysis',\n",
       " 'topic',\n",
       " 'modeling',\n",
       " 'topic',\n",
       " 'general',\n",
       " 'topic',\n",
       " 'model',\n",
       " 'built',\n",
       " 'tasa',\n",
       " 'touchstone',\n",
       " 'applied',\n",
       " 'science',\n",
       " 'associates',\n",
       " 'corpus',\n",
       " 'used',\n",
       " 'study',\n",
       " 'topic',\n",
       " 'scores',\n",
       " 'utterances',\n",
       " 'chat',\n",
       " 'data',\n",
       " 'computed',\n",
       " 'seven',\n",
       " 'relevant',\n",
       " 'topics',\n",
       " 'selected',\n",
       " 'based',\n",
       " 'total',\n",
       " 'document',\n",
       " 'scores',\n",
       " 'aggregated',\n",
       " 'topic',\n",
       " 'scores',\n",
       " 'power',\n",
       " 'predicting',\n",
       " 'students',\n",
       " 'learning',\n",
       " 'using',\n",
       " 'epistemic',\n",
       " 'network',\n",
       " 'analysis',\n",
       " 'enables',\n",
       " 'assessing',\n",
       " 'data',\n",
       " 'different',\n",
       " 'angle',\n",
       " 'results',\n",
       " 'showed',\n",
       " 'topic',\n",
       " 'score',\n",
       " 'based',\n",
       " 'epistemic',\n",
       " 'networks',\n",
       " 'gain',\n",
       " 'students',\n",
       " 'high',\n",
       " 'gain',\n",
       " 'students',\n",
       " 'significantly',\n",
       " 'different',\n",
       " 'overall',\n",
       " 'results',\n",
       " 'suggest',\n",
       " 'analytical',\n",
       " 'approaches',\n",
       " 'provide',\n",
       " 'complementary',\n",
       " 'information',\n",
       " 'afford',\n",
       " 'insights',\n",
       " 'processes',\n",
       " 'related',\n",
       " 'successful',\n",
       " 'collaborative',\n",
       " 'interactions',\n",
       " 'keywords',\n",
       " 'chat',\n",
       " 'collaborative',\n",
       " 'learning',\n",
       " 'topic',\n",
       " 'modeling',\n",
       " 'epistemic',\n",
       " 'network',\n",
       " 'analysis',\n",
       " 'introduction',\n",
       " 'collaborative',\n",
       " 'learning']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 17) print the first chunk, and compare it to the original text.\n",
    "# does that match what Sherin describes in his paper?\n",
    "#Yep! It looks pretty similar.\n",
    "hundred_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "\n",
      "\f",
      "epistemic network analysis and topic modeling for chat\n",
      "data from collaborative learning environment\n",
      "zhiqiang cai\n",
      "\n",
      "brendan eagan\n",
      "\n",
      "nia m. dowell\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "zcai@memphis.edu\n",
      "\n",
      "eaganb@gmail.com\n",
      "\n",
      "niadowell@gmail.com\n",
      "\n",
      "james w. pennebaker\n",
      "\n",
      "david w. shaffer\n",
      "\n",
      "arthur c. graesser\n",
      "\n",
      "university of texas-austin\n",
      "116 inner campus dr stop g6000\n",
      "austin, tx, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 403\n",
      "memphis, tn, usa\n",
      "\n",
      "pennebaker@utexas.edu\n",
      "\n",
      "dws@education.wisc.edu\n",
      "\n",
      "art.graesser@gmail.com\n",
      "\n",
      "abstract\n",
      "this study investigates a possible way to analyze chat data from\n",
      "collaborative learning environments using epistemic network\n",
      "analysis and topic modeling. a 300-topic general topic model\n",
      "built from tas\n"
     ]
    }
   ],
   "source": [
    "print(documents[0][:999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) how many chunks did Sherin have? What does a chunk become \n",
    "# in the next step of our topic modeling algorithm? \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He had 794 chunks of text. Next, each chunk of text is turned into a vector. The result is 794 vectors, each consisting of a list of 647 numbers. He then uses these to construct deviation vectors because the topics of each of the chunks is relatively similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) what are some other preprocessing steps we could do \n",
    "# to improve the quality of the text data? Mention at least 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We could remove verbs like 'using' or 'applied' or 'computed' - provided they describe the methodology\n",
    "2. We could remove headers ('abstract','keywords' etc)\n",
    "3. Perhaps we could have grouped like words together, for example words that are plurals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) in your own words, describe the next steps of the \n",
    "# data modeling algorithms (listed below):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector and Matrix Operations\n",
    "\n",
    "When you multiply vectors and matricies to get one matrix.\n",
    "\n",
    "#### Weight word frequency\n",
    "\n",
    "How many times words appeared to readers proportionate to how many times you used them in a particular text. \n",
    "\n",
    "#### Matrix normalization\n",
    "\n",
    "When you divide a vector by the magnitude so it has the same direction but the length is 1. \n",
    "\n",
    "#### Deviation vectors\n",
    "\n",
    "For two vectors, you find the average of the two vectors, and then you break each vector into a component that lies alont the average and another that is perpendicular to the average - this is the deviation vector becuase it commmunicates where two vectors deviate from one another. \n",
    "\n",
    "#### Clustering\n",
    "\n",
    "Organizing a set of objects into groups, so that the groups have similar features to each other. \n",
    "\n",
    "#### Visualizing the results\n",
    "\n",
    "Using charts, graphs or diagrams to visually display the results of your analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Vector and Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Weight word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Matrix normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Deviation Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Putting it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
